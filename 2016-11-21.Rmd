---
title: "STA257"
author: "Neil Montgomery | HTML is official | PDF versions good for in-class use only"
date: "Last edited: `r format(Sys.time(), '%Y-%m-%d %H:%M')`"
output: 
  ioslides_presentation: 
    css: '../styles.css' 
    widescreen: true 
    transition: 0.001
header-includes:
- \usepackage{cancel}
---
\newcommand\given[1][]{\:#1\vert\:}
\newcommand\P[1]{P{\left(#1\right)}}
\newcommand\F[1]{F_{\tiny{#1}}}
\newcommand\f[1]{f_{\tiny{#1}}}
\newcommand\p[1]{p_{\tiny{#1}}}
\newcommand\V[1]{\text{Var}\!\left(#1\right)}
\newcommand\E[1]{E\!\left(#1\right)}

## the classic density formulae: $W_1 = X + Y$

$$\begin{align*}
\f{W_1}(w) &= \int\limits_{-\infty}^\infty f(x, w-x)\,dx\\
&= \int\limits_{-\infty}^\infty \f{X}(x)\f{Y}(w-x)\,dx \quad \left(\text{when } X\perp Y\right)
\end{align*}$$
Proof:...

Example: $X \sim N(0,1)$ and $Y \sim N(0,1)$ with $X \perp Y$.

Example: $X \sim\text{Unif}[0,1]$ and $Y \sim\text{Unif}[0,1]$ with $X\perp Y$.

Exercise (textbook example): $X \sim \text{Exp}(\lambda)$ and $Y \sim \text{Exp}(\lambda)$ with $X \perp Y$

## the classic density formulae: $W_2 = Y\,/\,X$ { .smaller }

$$\begin{align*}
\f{W_2}(w) &= \int\limits_{-\infty}^\infty f(x, wx)|x|\,dx\\
&= \int\limits_{-\infty}^\infty \f{X}(x)\f{Y}(wx)|x|\,dx \quad \left(\text{when } X\perp Y\right)
\end{align*}$$

**NOTE ON PROOF IN CLASS**: My notes were correct. Something did not translate to the board. It turns out I used exactly the same method of proof as the textbook. So you can look on page 98 and everything is there. 

"**Mandatory**" exercise (done in book as example): $X \sim N(0,1)$ and $Y \sim N(0,1)$ with $X \perp Y$. This is a classic. We say $W_2$ has a Cauchy distribution with density $\frac{1}{\pi}\frac{1}{1+w^2}$

Example: $X \sim \text{Exp}(\lambda)$ and $Y \sim \text{Exp}(\lambda)$ with $X \perp Y$.

## the minumum and maximum of $n$ r.v.s { .build }

Practical example: A mine has 20 haul trucks with engines whose time-to-failure (from any cause) is random and with Exp$(\lambda)$ distribution. How long until the first failure of any truck?

Assumption: engines fail independently. 

Notation: $X_1, X_2, X_3,\ldots,X_{20}$ are independent (assumed) and have the same distribution Exp$(\lambda)$.

"i.i.d.": *independent and identically distributed*

Nothing special about 20, so let's consider the case of $X_1,\ldots,X_{n}$ i.i.d. Exp$(\lambda)$. The joint density will be:

$$f(x_1,\ldots,x_n) = \lambda e^{-\lambda x_1}\cdots\lambda e^{-\lambda x_n}= \lambda^ne^{-n\lambda \sum x_i}$$

## minimum of $n$ i.i.d. exponential r.v.s { .build }

Denote by $X_{(1)}$ the time to the first failure.

Theorem: $X_{(1)} \sim \text{Exp}(n\lambda).$

Proof: ...

Exercise (general case): If $X_1,\ldots,X_n$ are i.i.d. each with density $f(x)$ and cdf $F(x)$, the density of $X_{(1)} = \min\limits_{1\le i\le n}\{X_1, \ldots, X_n\}$ is:
$$\f{X_{(1)}}(x) = nf(x)[1-F(x)]^{n-1}$$

See section 3.7 up to the end of "E X A M P L E\ \ **B**"

Maximum is similar.

# expected value

## big money! { .build }

You and I agree to gamble on the outcome of one toss of one coin.

If $H$ appears, I give you $100. If $T$ appears, you give me $100.

This is a fair game. 

Denote by $Y$ my financial outcome. $X$ is discrete with pmf:
$$P(Y=y) = \begin{cases}
0.5 &: y=100\\
0.5 &: y=-100
\end{cases}$$

It's a fair game, so my "average" outcome should be 0. Otherwise it would not be rational for either of us to play the game!

This average is exactly $(100)(0.5) + (-100)(0.5) = 0.$

## expected value - discrete case

Definition: The expected value of $X$ that takes on values $\{x_1,x_2,\ldots\}$ with pmf $p(x)$ is:
$$E(X) = \sum_{i=1}^\infty x_ip(x_i)$$
provided $\sum\limits_{i=1}^\infty |x_i|p(x_i) < \infty$. Otherwise $E(X)$ is undefined.

## some of the "named" distributions

$X\sim\text{Bernoulli}(p).$ Then $E(X)=p$.

Proof: ...

$X\sim\text{Binomial}(n,p).$ Then $E(X) = np$.

Proof: ...

$X\sim\text{Geometric}(p).$ Then $E(X) = \frac{1}{p}$. 

Proof: ... (Use: $\frac{d}{dr}\sum\limits_{x=0}^\infty r^x = \sum\limits_{x=1}^\infty xr^{x-1}$ when $|r| < 1.$)

Exercise (book example): $X\sim\text{Poisson}(\lambda).$ Then $E(X) = \lambda$. 

## fun with expecations { .build }

Suppose $X$ has pmf $p(x) = \frac{6}{\pi^2x^2}$ on $x \in \{1,2,3,\ldots\}.$ 

Suppose $X$ has pmf $p(x) = \frac{3}{\pi^2x^2}$ on $x \in \{\pm1, \pm2, \pm3,\ldots\}.$

Treat $X$ as a constant, i.e. $X = a$. Then $E(X)=E(a)=a$.

For a sample space $S$ and event $A \subset S$, consider the "indicator" random variable $I_A.$ Then $E(I_A) = P(A).$

## expected value - continuous case { .build .smaller }

If $X$ is continuous with density $f$, its expected value is:

$$E(X) = \int\limits_{-\infty}^{\infty} xf(x)\,dx$$
provided $\int\limits_{-\infty}^{\infty} |x|f(x)\,dx < \infty$.

Examples: $U \sim \text{Unif}[a,b]$, $Z \sim N(0,1)$, $X$ Cauchy,...

**For a proof that the expected value (if it exists) of a random variable $X$ with density symmetric around $a$ must be $E(X)=a$, see Theorem 6.1.1 in the supplemental text: Schay (2016) *Introduction to Probability with Statistical Applications***

Exercise (textbook example): $X \sim \text{Gamma}(\alpha, \lambda)$

## optional: "meaning" of continuous definition { .smaller }

Not done in class. Take it or leave it.

The meaning of the discrete definition makes intuitive sense - $E(X)$ is the probability-weighted average of the possible $X$ outcomes.

The continuous definition can be motivated by the notion of "discretizing" the range of $X$ into a partition $\{x_1,x_2,\ldots\}.$ and considering the following approximation, which is again a probability-weighted average:
$$E(X)\approx\sum x_i P(x_i < X \le x_{i+1})$$
Draw a diagram of this to see what is going on. If the partition is fine enough we have $P(x_i < X \le x_{i+1})\approx f(x_i)(x_{i+1} - x_i) \approx f(x_i)\Delta x_i.$ Pass to the limit:
$$E(X)\approx\sum x_i P(x_i < X \le x_{i+1}) \approx \sum x_i f(x_i)\Delta x_i \longrightarrow \int x\,f(x)\,dx$$

